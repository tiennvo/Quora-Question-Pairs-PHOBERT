import streamlit as st
import helper
from sklearn.ensemble import RandomForestClassifier
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel
import faiss
import pandas as pd
import numpy as np

# Load Dataset
df = pd.read_csv('D:/Python/Quora/Quora/quora-question-pairs/Data/Dataset/train/dataset.csv')

# Load models
@st.cache_resource
def load_models():
    model_cls = AutoModelForSequenceClassification.from_pretrained("../phobert-finetuned")
    tokenizer = AutoTokenizer.from_pretrained("../phobert-finetuned")
    model_embed = AutoModel.from_pretrained("../phobert-embed")
    index = faiss.read_index("../faiss_index.bin")
    return model_cls, tokenizer, model_embed, index

model_cls, tokenizer, model_embed, index = load_models()

# H√†m d·ª± ƒëo√°n t∆∞∆°ng ƒë·ªìng
def predict_similarity(question1, question2):
    inputs = tokenizer(question1, question2, return_tensors="pt", truncation=True, padding=True, max_length=128)
    outputs = model_cls(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    return probs[0][1].item()

# H√†m encode ƒë·ªÉ t√¨m vector FAISS
def encode(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        outputs = model_embed(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()

def normalize(vec):
    norm = np.linalg.norm(vec)
    return vec if norm == 0 else vec / norm


# H√†m t√¨m c√¢u tr·∫£ l·ªùi g·∫ßn nh·∫•t
def find_answer_faiss(input_question, df):
    input_vector = normalize(encode(input_question)).astype('float32').reshape(1, -1)
    D, I = index.search(input_vector, 1)  # Top-1
    best_match = df.iloc[I[0][0]]
    return best_match['answer'] if D[0][0] > 0.6 else "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p."

def find_answer_faiss_all(input_question, df, threshold=0.6, top_k=5):
    # M√£ h√≥a v√† chu·∫©n h√≥a c√¢u h·ªèi ƒë·∫ßu v√†o
    input_vector = normalize(encode(input_question)).astype('float32').reshape(1, -1)

    # T√¨m top_k c√¢u h·ªèi g·∫ßn nh·∫•t theo cosine similarity
    D, I = index.search(input_vector, top_k)

    # T√¨m c√¢u c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t trong top_k v√† > threshold
    best_idx = None
    best_score = -1

    for score, idx in zip(D[0], I[0]):
        if score > threshold and score > best_score:
            best_score = score
            best_idx = idx

    # Tr·∫£ l·∫°i k·∫øt qu·∫£ n·∫øu t√¨m th·∫•y
    if best_idx is not None:
        return df.iloc[best_idx]['answer']
    else:
        return "Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p."


# Giao di·ªán Streamlit
st.title("Quora Question Pairs")

question1 = st.text_input("Nh·∫≠p C√¢u h·ªèi 1")
question2 = st.text_input("Nh·∫≠p C√¢u h·ªèi 2")

if st.button("üîç Ki·ªÉm tra"):
    q1 = helper.preprocess(question1)
    q2 = helper.preprocess(question2)
    score = predict_similarity(q1, q2)

    if score > 0.6:
        answer = find_answer_faiss(q1, df)
        st.success(f"‚úÖ Duplicate (Score: {round(score * 100)}%)")
        st.markdown(f"**G·ª£i √Ω tr·∫£ l·ªùi:** {answer}")
    else:
        st.warning(f"‚ùå Not Duplicate (Score: {round(score * 100)}%)")
