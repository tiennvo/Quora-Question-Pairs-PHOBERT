{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import gensim.downloader as api\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from PIL import Image, ImageTk\n",
    "import re\n",
    "import jellyfish\n",
    "from typing import List, Set\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_event = threading.Event()\n",
    "# Kiểu mô hình mặc định\n",
    "default_model = \"PhoBERT\"\n",
    "model = None\n",
    "tokenizer = None\n",
    "word2vec_model = None\n",
    "doc2vec_model = None\n",
    "test1_path = None\n",
    "test2_path = None\n",
    "# độ dài mã thông báo\n",
    "MAX_LENGTH = 512\n",
    "local_model_dir = \"./models\"\n",
    "matrix_file_path1 = None\n",
    "matrix_file_path2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'vietnamese-stopwords.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    STOP_WORDS = set(file.read().splitlines())\n",
    "\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Python/Quora/Quora/quora-question-pairs/Data/Dataset/train/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(q):\n",
    "    \n",
    "    q = str(q).lower().strip()\n",
    "    \n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "    \n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "    \n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "    \n",
    "    contractions = { \n",
    "    \"sp\": \"sư phạm\",\n",
    "    \"sn\": \"sinh năm\",\n",
    "    \"bn\": \"bao nhiêu\",\n",
    "    \"ngta\": \"người ta\",\n",
    "    \"lm\": \"làm\",\n",
    "    \"khg\": \"không\",\n",
    "    \"ko\": \"không\",\n",
    "    \"hok\": \"không\",\n",
    "    \"khum\": \"không\",\n",
    "    \"kg\": \"không\",\n",
    "    \"đc\": \"được\",\n",
    "    \"dc\": \"được\",\n",
    "    \"đg\": \"đang\",\n",
    "    \"ng\": \"người\",\n",
    "    \"bt\": \"biết\",\n",
    "    \"h\": \"giờ\",\n",
    "    \"hc\": \"học\",\n",
    "    \"vt\": \"viết\",\n",
    "    \"vc\": \"việc\",\n",
    "    \"trc\": \"trước\",\n",
    "    \"j\": \"gì\",\n",
    "    \"xg\": \"xong\",\n",
    "    \"bx\": \"bữa\",\n",
    "    \"vg\": \"vâng\",\n",
    "    \"v\": \"vậy\",\n",
    "    \"m\": \"mày\",\n",
    "    \"t\": \"tôi\",\n",
    "    \"s\": \"sao\",\n",
    "    \"r\": \"rồi\",\n",
    "    \"chs\": \"chơi\",\n",
    "    \"ae\": \"anh em\",\n",
    "    \"a\": \"anh\",\n",
    "    \"e\": \"em\",\n",
    "    \"cj\": \"chị\",\n",
    "    \"đhsp\": \"đại học sư phạm\",\n",
    "    \"đh\": \"đại học\",\n",
    "    \"đhspkt\": \"đại học sư phạm kỹ thuật\",\n",
    "    \"ktx\": \"kí túc xá\",\n",
    "    \"đt\": \"điện thoại\",\n",
    "    \"cntt\": \"công nghệ thông tin\",\n",
    "    \"tp\": \"thành phố\",\n",
    "    \"hcm\": \"hồ chí minh\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "\n",
    "        q_decontracted.append(word)\n",
    "\n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    q = BeautifulSoup(q)\n",
    "    q = q.get_text()\n",
    "    # Remove punctuations\n",
    "    pattern = re.compile('\\W')\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['question1'] = new_df['question1'].apply(preprocess)\n",
    "new_df['question2'] = new_df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir_exists(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_dir_exists(local_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_download_phobert():\n",
    "    phobert_model_path = os.path.join(local_model_dir, \"models--vinai--phobert-base\")\n",
    "    if not os.path.exists(phobert_model_path):\n",
    "        print(\"Downloading PhoBERT model...\")\n",
    "        AutoModel.from_pretrained(\"vinai/phobert-base\", cache_dir=local_model_dir)\n",
    "        AutoTokenizer.from_pretrained(\"vinai/phobert-base\", cache_dir=local_model_dir)\n",
    "    else:\n",
    "        print(\"PhoBERT model loaded from local storage.\")\n",
    "    return AutoModel.from_pretrained(\n",
    "        \"vinai/phobert-base\", cache_dir=local_model_dir\n",
    "    ), AutoTokenizer.from_pretrained(\"vinai/phobert-base\", cache_dir=local_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_models():\n",
    "    global model, tokenizer\n",
    "    try:\n",
    "        if not os.path.exists(local_model_dir):\n",
    "            os.makedirs(local_model_dir)\n",
    "            print(f\"Directory created: {local_model_dir}\")\n",
    "        ensure_dir_exists(local_model_dir)\n",
    "        model, tokenizer = load_or_download_phobert()\n",
    "        print(f\"Models loaded successfully.\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chuẩn hóa ma trận tương đồng về khoảng [0, 1].\n",
    "def normalize_similarity_matrix(matrix):\n",
    "    return (matrix + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trích xuất embedding của câu bằng PhoBERT.\n",
    "def get_embedding(sentence):\n",
    "    inputs = tokenizer(\n",
    "        sentence, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tính độ tương đồng cosine giữa hai câu.\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    embed1 = get_embedding(sent1)\n",
    "    embed2 = get_embedding(sent2)\n",
    "    cosine_similarity = np.dot(embed1, embed2) / (\n",
    "        np.linalg.norm(embed1) * np.linalg.norm(embed2)\n",
    "    )\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tính độ tương đồng sử dụng PhoBERT.\n",
    "def calculate_similarity_phobert(questions):\n",
    "    try:\n",
    "        normalized_similarity_matrix = sentence_similarity(questions[0], questions[1])\n",
    "        return normalized_similarity_matrix\n",
    "    except Exception as e:\n",
    "\n",
    "        raise RuntimeError(f\"Error calculate similarity Phobert: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gọi hàm tính toán độ tương đồng tùy thuộc vào loại mô hình.\n",
    "def calculate_similarity(questions, model_type):\n",
    "    questions1 = questions.copy()\n",
    "    if model_type == \"PhoBERT\":\n",
    "        return calculate_similarity_phobert(questions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khai báo biến toàn cục\n",
    "data_file_path1 = \"\"\n",
    "data_file_path2 = \"\"\n",
    "keywords_file_path = \"\"\n",
    "matrix_tab3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đọc từ khóa từ file Excel\n",
    "def load_keywords_from_excel(\n",
    "    file_path: str, sheet_name: str, column_name: str\n",
    ") -> Set[str]:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    keywords = set(df[column_name].dropna().str.strip().str.lower())\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm kiểm tra từ khóa dựa trên ngưỡng tương đồng Jaro-Winkler\n",
    "def is_keyword_present_with_threshold(\n",
    "    keyword: str, words: List[str], threshold: float = 0.6\n",
    ") -> int:\n",
    "    for word in words:\n",
    "        similarity = jellyfish.jaro_winkler_similarity(keyword, word)\n",
    "        if similarity >= threshold:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tạo véc-tơ đặc trưng cho câu hỏi\n",
    "def create_feature_vector(\n",
    "    words: List[str], keywords: Set[str], threshold: float = 0.6\n",
    ") -> List[int]:\n",
    "    feature_vector = [\n",
    "        is_keyword_present_with_threshold(keyword, words, threshold)\n",
    "        for keyword in keywords\n",
    "    ]\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm tính độ tương đồng Jaccard\n",
    "def jaccard_similarity(vec1: List[int], vec2: List[int]) -> float:\n",
    "    intersection = sum(1 for v1, v2 in zip(vec1, vec2) if v1 == 1 and v2 == 1)\n",
    "    union = sum(1 for v1, v2 in zip(vec1, vec2) if v1 == 1 or v2 == 1)\n",
    "    return intersection / union if union != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(new_df):\n",
    "    inputs = tokenizer(\n",
    "        new_df['question1'].tolist(),\n",
    "        new_df['question2'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs['labels'] = new_df['is_duplicate'].tolist()\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áp dụng tiền xử lý\n",
    "train_data = preprocess_data(train_df)\n",
    "test_data = preprocess_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionPairDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo Dataset\n",
    "train_dataset = QuestionPairDataset(train_data, train_df['is_duplicate'].tolist())\n",
    "test_dataset = QuestionPairDataset(test_data, test_df['is_duplicate'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mô hình PhoBERT với đầu ra là phân loại nhị phân\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=2)\n",
    "model_embed = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "# Thiết lập optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available and being used.\")\n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(5):  # Số epoch\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#đánh giá mô hình\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./phobert-finetuned\")\n",
    "tokenizer.save_pretrained(\"./phobert-finetuned\")\n",
    "model_embed.save_pretrained(\"./phobert-embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import faiss\n",
    "\n",
    "# Tải lại mô hình và tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./phobert-finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./phobert-finetuned\")\n",
    "model_embed = AutoModel.from_pretrained(\"./phobert-embed\")\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Dự đoán độ tương đồng\n",
    "def predict_similarity(question1, question2):\n",
    "    inputs = tokenizer(question1, question2, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = torch.softmax(outputs.logits, dim=1)\n",
    "    return probs[0][1].item()  # Xác suất thuộc lớp tương đồng (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(input_question, dataset, threshold=0.6):\n",
    "    # Tính độ tương đồng giữa câu hỏi đầu vào và các câu hỏi trong bộ dữ liệu\n",
    "    max_similarity = -1\n",
    "    best_match = None\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Tính độ tương đồng giữa input_question và question1\n",
    "        similarity1 = predict_similarity(input_question, row['question1'])\n",
    "        # Tính độ tương đồng giữa input_question và question2\n",
    "        similarity2 = predict_similarity(input_question, row['question2'])\n",
    "        # Lấy độ tương đồng cao nhất\n",
    "        max_current_similarity = max(similarity1, similarity2)\n",
    "\n",
    "        # Nếu độ tương đồng cao hơn ngưỡng và cao hơn giá trị hiện tại\n",
    "        if max_current_similarity > threshold and max_current_similarity > max_similarity:\n",
    "            max_similarity = max_current_similarity\n",
    "            best_match = row\n",
    "\n",
    "    # Nếu tìm thấy câu hỏi trùng khớp, trả về câu trả lời\n",
    "    if best_match is not None:\n",
    "        return best_match['answer']\n",
    "    else:\n",
    "        return \"Không tìm thấy câu trả lời phù hợp.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "def encode(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_embed(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()  # Chuyển về 1D\n",
    "\n",
    "# Tạo danh sách vector từ các câu hỏi\n",
    "question_vectors = np.array([encode(row['question1']) for _, row in df.iterrows()])\n",
    "\n",
    "# Kiểm tra kích thước\n",
    "print(question_vectors.shape)  # Kết quả phải là (số lượng câu, 768)\n",
    "\n",
    "# Khởi tạo FAISS Index\n",
    "index = faiss.IndexFlatL2(question_vectors.shape[1])\n",
    "index.add(question_vectors)  # Không còn lỗi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu FAISS index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "# Lưu các câu hỏi dưới dạng numpy để khôi phục sau này\n",
    "np.save(\"questions.npy\", df[['question1', 'question2', 'answer']].to_numpy(), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_answer_faiss(input_question, df):\n",
    "    input_vector = encode(input_question).astype('float32')\n",
    "    input_vector = input_vector.reshape(1, -1)\n",
    "    D, I = index.search(input_vector, 1)  # Tìm 1 câu gần nhất\n",
    "    best_match = df.iloc[I[0][0]]\n",
    "    return best_match['answer'] if D[0][0] < 0.6 else \"Không tìm thấy câu trả lời phù hợp.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ví dụ\n",
    "question1 = \"Trang Facebook nào nhất ở Ấn Độ?\"\n",
    "question2 = \"Khi nào Facebook ra mắt ở Ấn Độ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = new_df.head(5)\n",
    "test_df['similarity_score'] = test_df.apply(lambda row: predict_similarity(row['question1'], row['question2']), axis=1)\n",
    "test_df['answer_question'] = test_df.apply(lambda row: find_answer_faiss(row['question1'], df), axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
